{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the data: transforming xlsx into a jsonl file with the specified format of model input"
      ],
      "metadata": {
        "id": "9Ef06oZHY8JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first of all in google colab, change runtime into A100 gpu, keep trying until gpu is free and can be allocated\n",
        "\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "\n",
        "#real excel file and apply first transformation in the format ###Human\\n\\n###Assistant\n",
        "df = pd.read_excel(\"/content/Q_A.xlsx\")\n",
        "\n",
        "df[\"text\"] = df.apply(lambda row: f\"###Human:{row['Question']}\\n\\n###Assistant:\\n {row['Answer']}\", axis=1).apply(lambda text: f'\"{text}\"')\n",
        "\n",
        "df.to_excel(\"data_with_text.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "crRaNYPcYcTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install files"
      ],
      "metadata": {
        "id": "_PoLZTcDZ75c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xlrd\n",
        "\n",
        "# Read the Excel file into a Pandas DataFrame\n",
        "df = pd.read_excel('/content/data_with_text.xlsx')\n",
        "\n",
        "# Keep only the first column\n",
        "df = df[['text']]\n",
        "\n",
        "# Write the DataFrame to a new file\n",
        "df.to_excel('new_excel_file.xlsx', index=False)"
      ],
      "metadata": {
        "id": "gy7F_gcVaAk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Read the Excel spreadsheet.\n",
        "df = pd.read_excel('/content/new_excel_file.xlsx')\n",
        "\n",
        "# Get the text column.\n",
        "text_column = df['text']\n",
        "\n",
        "# Convert the text column to a list of JSON objects.\n",
        "json_lines = [json.dumps({'text': line}) for line in text_column]\n",
        "\n",
        "# Save the JSON lines to a file.\n",
        "with open('json_lines.jsonl', 'w') as f:\n",
        "    f.writelines(json_lines)"
      ],
      "metadata": {
        "id": "blfDITVuaGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "ZeGmxbf5abBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('json', data_files='/content/json_lines.jsonl', split='train')\n",
        "#dataset = pd.read_excel(\"/content/new_excel_file.xlsx\")\n",
        "\n",
        "\n",
        " #Shuffle the dataset and slice it\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Define a function to transform the data\n",
        "def transform_conversation(example):\n",
        "    conversation_text = example['text']\n",
        "    segments = conversation_text.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over pairs of segments\n",
        "    for i in range(1, len(segments) - 1, 2):\n",
        "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
        "\n",
        "        # Check if there is a corresponding assistant segment before processing\n",
        "        if i + 1 < len(segments):\n",
        "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
        "\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_dataset = dataset.map(transform_conversation)\n"
      ],
      "metadata": {
        "id": "aqsShtwfao4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the transformed dataset to a file.\n",
        "transformed_dataset.save_to_disk('transformed_dataset.jsonl')"
      ],
      "metadata": {
        "id": "r6H-3zXcarbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Login now into huggingface account to make the liaison"
      ],
      "metadata": {
        "id": "eKRD_UX0auGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface-hub"
      ],
      "metadata": {
        "id": "_ewyCugJa5Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "xnyAFglRa7Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing librairies and training requirements"
      ],
      "metadata": {
        "id": "4Sun-YMQbEGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate"
      ],
      "metadata": {
        "id": "RJTlpc3Taa6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "id": "HDzPqyajajpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "1b3dtDukanjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "Jk_fNThobAEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing training parameters"
      ],
      "metadata": {
        "id": "bMZFLPf_bSQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the model to train from hugging face hub\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "#the instruction dataset to use\n",
        "dataset_name=dataset\n",
        "\n",
        "#fine-tuned model name\n",
        "new_model=\"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = True\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "phaSRs5xbV11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Starting training with A100GPU"
      ],
      "metadata": {
        "id": "pDQoNQebbf2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "#dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=transformed_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "3gY5rfQTbwm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the model inference"
      ],
      "metadata": {
        "id": "hSbLzexrcNC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"How do comfort and satisfaction measurements impact worker morale and productivity?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "pakfFS6fcQvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Returning back the model to 16bits precision after converting to 4bits during training"
      ],
      "metadata": {
        "id": "5DMAbSxncXfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mR55-sRncfR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "UujEJpgWchKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "05isYr2kciok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting drive and uploading model to it"
      ],
      "metadata": {
        "id": "oyva9jNHkPU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#due to the need of the session neeeds to be still on while downloadinf model and that the gpu would still be running,\n",
        "#it is best to upload model to personal drive and download it in our own time that way we can shut down already the gpu and reduce costs\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/my_model.pth'\n",
        "\n",
        "#torch.save(model, model_path) this one donwloads it as unique file .pth\n",
        "#this one save it as multiple files containing tensors that we can use later for converting\n",
        "model.save_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "u68u5s_PckeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#After uploading model to drive and terminating gpu session we start downloading to azure directory for converting using the script download large file to azure"
      ],
      "metadata": {
        "id": "lppjRpwxdNwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "def authenticate_and_get_drive():\n",
        "    \"\"\"\n",
        "    Authenticates with Google and returns a GoogleDrive instance.\n",
        "\n",
        "    Returns:\n",
        "    - A GoogleDrive instance if authentication is successful, None otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.CommandLineAuth()  # Creates a link for authentication\n",
        "        drive = GoogleDrive(gauth)\n",
        "        print(\"Authentication successful.\")\n",
        "        return drive\n",
        "    except Exception as e:\n",
        "        print(f\"Error authenticating: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_file_from_drive(drive, file_id, destination_path):\n",
        "    \"\"\"\n",
        "    Downloads a file from Google Drive.\n",
        "\n",
        "    Args:\n",
        "    - drive: A GoogleDrive instance.\n",
        "    - file_id: The ID of the file to be downloaded.\n",
        "    - destination_path: The local path where the file will be downloaded to.\n",
        "\n",
        "    Returns:\n",
        "    - True if download is successful, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_to_download = drive.CreateFile({'id': file_id})\n",
        "        file_to_download.GetContentFile(destination_path)\n",
        "        print(f\"File downloaded successfully from Google Drive: {destination_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        return False\n",
        "\n",
        "# Authenticate with Google\n",
        "drive = authenticate_and_get_drive()\n",
        "\n",
        "if drive:\n",
        "    # Example: File ID of the file to be downloaded from Google Drive\n",
        "    file_id_to_download = \"1-0Cz3ctbeGBYxUub0Z1QmDmGE7HTfYDA\"\n",
        "\n",
        "    # Example: Local path where the file will be downloaded to\n",
        "    downloaded_file_path = \"filed.bin\"\n",
        "\n",
        "    # Download the file from Google Drive\n",
        "    download_success = download_file_from_drive(drive, file_id_to_download, downloaded_file_path)\n",
        "\n",
        "    if not download_success:\n",
        "        print(\"File download failed.\")\n",
        "else:\n",
        "    print(\"Authentication failed. Cannot download file.\")"
      ],
      "metadata": {
        "id": "VUELT16ndUL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Before using any script that requires any interaction with google drive, we must follow these steps to set up the liaison between drive and azure."
      ],
      "metadata": {
        "id": "zK_5mQ9iSspf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "library to install:\n",
        "\n",
        "pip install httplib2==0.15.0\n",
        "\n",
        "pip install PyDrive\n",
        "\n",
        "in google clouse console:\n",
        "\n",
        "https://console.cloud.google.com/\n",
        "\n",
        "Api and Services - library - search for google drive API - activate API\n",
        "\n",
        "0auth - create new application - put application on test - add the email adress as test user\n",
        "\n",
        "ps: copy file id from drive and put on script to download and same for upload copy file path"
      ],
      "metadata": {
        "id": "2DzxVdnCdkaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next setting up machine learning environment in Azure"
      ],
      "metadata": {
        "id": "fRHZ6VFed4QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-creating Azure machine learning workspace\n",
        "\n",
        "2-download config.json file that contains all the information we will need about the workspace\n",
        "\n",
        "3-launch machine learning studio\n",
        "\n",
        "4-go Assets/Models and use deploy a custom model.\n",
        "\n",
        "5-since a model trained on gpu can only be deployed on a gpu, we neeed to convert it first into a cpu compatible model .\n",
        "\n",
        "6- after that finished, go Authoring/Notebook and create a new ipynb notebookfile"
      ],
      "metadata": {
        "id": "u5vPiTWsfsIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#this code if we need to create a compute instance and choose a cpu VM then we open terminal and prepare onnx machine(onnx doesn't work for llm for now)"
      ],
      "metadata": {
        "id": "mn9SLwknhmbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#codes to prepare onnx environment\n",
        "\n",
        "conda create -n cpu_env_demo python=3.8\n",
        "conda activate cpu_env_demo\n",
        "conda install -c anaconda ipykernel\n",
        "conda install -c conda-forge ipywidgets\n",
        "python -m ipykernel install --user --name=cpu_env_demo\n",
        "jupyter notebook"
      ],
      "metadata": {
        "id": "xLtZfTMmhz7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# this code is used for uploading a large file to azure notebook directory since it is not permitted we use a trick to upload that file as a custom model then call it to the notebook directory using this code."
      ],
      "metadata": {
        "id": "G33gP5U2g50P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\n",
        "import azureml.core\n",
        "\n",
        " Connect to your Azure Machine Learning workspace\n",
        "ws = azureml.core.Workspace.from_config()\n",
        "\n",
        " Load the registered model\n",
        "model = Model(workspace=ws, name=\"llama2-industryx0\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v4DXFUfhfcBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test if model is loaded\n",
        "\n",
        "if model is not None:\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(\"Model failed to load.\")"
      ],
      "metadata": {
        "id": "TkAwJSPWg3ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after loading it the notebook, we need to download it in the notebook directory\n",
        "\n",
        "model.download()\n"
      ],
      "metadata": {
        "id": "QVn1_YSUhQdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next we install libraires required for the conversion to an onnx model format(if we needed onxy for another non LLM model)"
      ],
      "metadata": {
        "id": "Iylz1IEohdQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install different requirements\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "_zOroV5cid9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, datasets, transforms as T\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ocxh8rTkihke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "jiEVSaQBijOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")"
      ],
      "metadata": {
        "id": "E1tJ7nTZim3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install huggingface-hub"
      ],
      "metadata": {
        "id": "aKQEwwEtioq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "u8Ba5-QliqGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ],
      "metadata": {
        "id": "d9hEyOB0irtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install peft"
      ],
      "metadata": {
        "id": "E8xl6rZWis--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install trl"
      ],
      "metadata": {
        "id": "L0-Ah_YdiufC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "uL_vpTkbiv7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next, we load model from notebook directory"
      ],
      "metadata": {
        "id": "uhr--oC-izRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the model\n",
        "model = torch.load('my_model.pth', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "H1kO3i_di7GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model is not None:\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(\"Model failed to load.\")"
      ],
      "metadata": {
        "id": "yLY_w43Gi9TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model type need to be pytorch type\n",
        "model_type = type(model)\n",
        "print(model_type)"
      ],
      "metadata": {
        "id": "UB_g046fi-6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lastly, we prepare the conversion functions parameters and start conversion"
      ],
      "metadata": {
        "id": "CVNQ0Un4jGWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Your input text goes here.\"\n",
        "tokenized_input = tokenizer.encode(input_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "EBKc_5lYjPUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model need to be converted first to float32\n",
        "model = model.to(torch.float32)\n"
      ],
      "metadata": {
        "id": "PxpopU_QjRKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install onnx"
      ],
      "metadata": {
        "id": "cHb_QfDsjS5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversion code to onnx model\n",
        "torch.onnx.export(model, tokenized_input, \"mymodelonnx.onnx\", verbose=True, input_names=[\"input\"], output_names=[\"output\"])\n"
      ],
      "metadata": {
        "id": "8toR847rjUZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now in this case we won't use onnx and uploading locally from machine, instead we will use the file imported from drive using the script from earlier and we start preparing for the conversion using llama.cpp"
      ],
      "metadata": {
        "id": "xyDHeHo5Rpj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first step, we clone the folder containing the conversion tool llama.cpp\n",
        "git clone https://github.com/strutive07/llama.cpp.git\n"
      ],
      "metadata": {
        "id": "mQ_NYl4vSHEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the we use the script file in the folder with this command\n",
        "python /path/to/llama.cpp/convert.py results/merged_model/ --outtype f16 --outfile results/merged.bin --vocab-dir meta-llama/Llama-2-7b-hf --vocabtype hf\n",
        "#ps: anymissing file needed for conversion, we can download directly from the model directory in huggingface"
      ],
      "metadata": {
        "id": "W-djyF_3WzYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#After the conversion is finished, we need gpu in order to do the quantization so we need to move back the model to google drive using the script uplargefile to drive."
      ],
      "metadata": {
        "id": "MY4_VHC8XlaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_file(source_path, output_path):\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(source_path, arcname=os.path.basename(source_path))\n",
        "\n",
        "def upload_zipped_file_to_drive(drive, zipped_file_path, title=None, chunk_size=None):\n",
        "    try:\n",
        "        file_metadata = {'title': title if title else os.path.basename(zipped_file_path)}\n",
        "        file_to_upload = drive.CreateFile(file_metadata)\n",
        "\n",
        "        # Set content and chunk size before uploading\n",
        "        file_to_upload.SetContentFile(zipped_file_path)\n",
        "\n",
        "        if chunk_size:\n",
        "            file_to_upload.Upload(param={'uploadType': 'resumable'})\n",
        "\n",
        "            # Manually set the Content-Range header\n",
        "            headers = {'Content-Range': 'bytes 0-' + str(chunk_size - 1) + '/' + str(os.path.getsize(zipped_file_path))}\n",
        "            file_to_upload._BuildMultipleUploadParam({}, headers)\n",
        "        else:\n",
        "            file_to_upload.Upload()\n",
        "\n",
        "        print(f\"File uploaded successfully to Google Drive: {file_to_upload['title']}\")\n",
        "        return file_to_upload\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Assuming gauth is your authenticated GoogleAuth instance\n",
        "gauth = GoogleAuth()\n",
        "gauth.CommandLineAuth()  # Creates a link for authentication\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Example: Local path of the file to be uploaded\n",
        "original_file_path = \"merged.bin\"\n",
        "zipped_file_path = \"merged.zip\"\n",
        "\n",
        "# Zip the file\n",
        "zip_file(original_file_path, zipped_file_path)\n",
        "\n",
        "# Example: Title to be used for the uploaded file on Google Drive (optional)\n",
        "uploaded_file_title = \"config_merged.zip\"\n",
        "\n",
        "# Upload the zipped file to Google Drive with optional chunk size\n",
        "# Example chunk size: 10 MB (10 * 1024 * 1024 bytes)\n",
        "uploaded_file = upload_zipped_file_to_drive(drive, zipped_file_path, title=uploaded_file_title, chunk_size=10*1024*1024)\n",
        "\n",
        "if uploaded_file is None:\n",
        "    print(\"File upload failed.\")\n",
        "else:\n",
        "    print(\"File uploaded successfully.\")\n"
      ],
      "metadata": {
        "id": "vOUrFY5sX4Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#After we have the file in google colab, we can start the quantization process using those commands also contained in the script file quantization"
      ],
      "metadata": {
        "id": "_XzCTMLwYDn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ff7P7iG-YSan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd drive/MyDrive\n"
      ],
      "metadata": {
        "id": "Z4BpLN5qYWrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/merged2.zip\n"
      ],
      "metadata": {
        "id": "QYhxC4i1YjNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "Q4ciRIgSYnDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp"
      ],
      "metadata": {
        "id": "bs2FpElbYpho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install build-essential git cmake libopenblas-dev libeigen3-dev"
      ],
      "metadata": {
        "id": "yqtNyRn7Yt7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp/ ; sudo make LLAMA_OPENBLAS=1"
      ],
      "metadata": {
        "id": "soLP_u-QYxPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "Xzb73DuHY1iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update ; sudo apt-get install --only-upgrade cmake"
      ],
      "metadata": {
        "id": "yVpX5IPMY5Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/llama.cpp/build ; rm -rf CMakeCache.txt"
      ],
      "metadata": {
        "id": "Q55MCodAY9LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/llama.cpp/build ; sudo cmake .. -DCMAKE_BUILD_TYPE=Release ; sudo make"
      ],
      "metadata": {
        "id": "kQLK5no-ZBhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/llama.cpp/build ; sudo cmake --build . --config Release --no-cache"
      ],
      "metadata": {
        "id": "A2RfPJkTZD72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "8-LM0NZyZKJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/quantize /content/merged.bin /content/merged_ggml_q8_0.bin q8_0\n"
      ],
      "metadata": {
        "id": "copcW0MnZPZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/merged_ggml_q8_0.bin'  # Replace with the actual path to your file\n",
        "destination_path = '/content/drive/MyDrive/filex.bin'  # Change the destination as needed\n",
        "\n",
        "shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "id": "LFAPHQXZZXTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/ ; ls -lh\n"
      ],
      "metadata": {
        "id": "jXsca8-AZaWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Since the final quantized model is around 6gb we can actually download it from drive and upload it the normal way using model/register as custom and we use the respctive environement(docker file) and the upload script."
      ],
      "metadata": {
        "id": "YypdvOdWZp1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is docker file\n",
        "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n",
        "RUN pip install azureml-mlflow\n",
        "RUN pip install azureml-core\n",
        "RUN pip install azureml-defaults\n",
        "RUN pip install azure-ai-ml\n",
        "RUN pip install numpy\n",
        "RUN pip install pandas\n",
        "RUN pip install torch\n",
        "RUN pip install langchain\n",
        "RUN pip install ctransformers\n",
        "RUN apt-get update\n",
        "RUN apt-get -y upgrade\n",
        "RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
      ],
      "metadata": {
        "id": "0eWOji42aDLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is deployement script\n",
        "import os\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "from langchain.llms import CTransformers\n",
        "\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "\n",
        "\n",
        "# Handler that prints each new token as it is computed\n",
        "\n",
        "class NewTokenHandler(BaseCallbackHandler):\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "\n",
        "        print(f\"{token}\", end=\"\", flush=True)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the Llama model outside the run function\n",
        "\n",
        "model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"filex.bin\")\n",
        "\n",
        "llm = CTransformers(\n",
        "\n",
        "    model=model_path,  # Location of downloaded GGML model\n",
        "\n",
        "    model_type=\"llama\",  # Model type Llama\n",
        "\n",
        "    stream=True,\n",
        "\n",
        "    callbacks=[NewTokenHandler()],\n",
        "\n",
        "    config={'max_new_tokens': 256, 'temperature': 0.3}\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def init():\n",
        "\n",
        "    # This function is called when the container is started\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "def run(input_data):\n",
        "\n",
        "    # This function is called for each batch of input data\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Assume your input data is in JSON format\n",
        "\n",
        "        input_data = json.loads(input_data)\n",
        "\n",
        "\n",
        "\n",
        "        # Extract the prompt from input data\n",
        "\n",
        "        prompt = input_data.get(\"prompt\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "        # Generate text using the llm\n",
        "\n",
        "        output = llm(prompt)\n",
        "\n",
        "\n",
        "\n",
        "        # Format the output\n",
        "\n",
        "        output_data = {\"generated_text\": output}\n",
        "\n",
        "\n",
        "\n",
        "        return json.dumps(output_data)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        error = str(e)\n",
        "\n",
        "        return json.dumps({\"error\": error})"
      ],
      "metadata": {
        "id": "hCnIySxsaSCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}